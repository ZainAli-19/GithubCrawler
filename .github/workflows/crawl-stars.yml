name: Crawl 100K GitHub Repositories

on:
  workflow_dispatch:          # ‚úÖ Manual trigger
  schedule:
    - cron: "0 0 * * *"       # ‚úÖ Daily at midnight UTC

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 360      # Allow up to 6 hours

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      POSTGRES_HOST: 127.0.0.1
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
      PGPASSWORD: postgres
      GITHUB_TOKEN: ${{ github.token }}

    steps:
      # 1Ô∏è‚É£ Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2Ô∏è‚É£ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3Ô∏è‚É£ Install dependencies
      - name: Install dependencies
        run: pip install -r src/requirements.txt

      # 4Ô∏è‚É£ Wait for PostgreSQL
      - name: Wait for PostgreSQL
        run: |
          echo "‚è≥ Waiting for PostgreSQL..."
          for i in {1..20}; do
            if pg_isready -h 127.0.0.1 -p 5432 -U postgres; then
              echo "‚úÖ PostgreSQL is ready!"
              break
            fi
            echo "PostgreSQL not ready yet... retrying in 3s"
            sleep 3
          done

      # 5Ô∏è‚É£ Setup schema
      - name: Setup database schema
        run: |
          echo "üì¶ Applying database migrations..."
          psql -h 127.0.0.1 -U postgres -d postgres -f src/db/migrations/001_create_tables.sql
          echo "‚úÖ Schema ready."

      # 6Ô∏è‚É£ Run async parallel crawler until DB >= 100K repos
      - name: Run async parallel crawler until 100K
        working-directory: src
        run: |
          TARGET=100000
          ATTEMPT=1
          export PGPASSWORD=$POSTGRES_PASSWORD
          while true; do
            echo "üöÄ Run #$ATTEMPT ‚Äî running crawler_async.py ..."
            python -m crawler.crawler_async || true

            COUNT=$(psql -h 127.0.0.1 -U postgres -d postgres -t -c "SELECT COUNT(*) FROM repositories;" | tr -d '[:space:]')
            echo "üìä Current repositories in DB: $COUNT"

            if [ -n "$COUNT" ] && [ "$COUNT" -ge "$TARGET" ]; then
              echo "üéØ Target reached ‚Äî $COUNT repositories collected."
              break
            fi

            ATTEMPT=$((ATTEMPT + 1))
            echo "‚è≥ Not yet 100K (current: $COUNT). Retrying after 2 minutes..."
            sleep 120
          done

          echo "‚úÖ Final count: $(psql -h 127.0.0.1 -U postgres -d postgres -t -c 'SELECT COUNT(*) FROM repositories;')"

      # 7Ô∏è‚É£ Export Database Snapshot to CSV
      - name: Export repositories to CSV
        run: |
          mkdir -p data
          echo "üì§ Exporting repositories table..."
          psql -h 127.0.0.1 -U postgres -d postgres -c "\copy (SELECT * FROM repositories ORDER BY stars DESC) TO 'data/repos.csv' CSV HEADER"
          echo "‚úÖ Export complete."

      # 8Ô∏è‚É£ Commit and Push CSV
      - name: Commit and push updated data
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add data/repos.csv
          if git diff --cached --quiet; then
            echo "üü° No new data to commit."
          else
            git commit -m "üìä Auto-update: 100K repo crawl on $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
            git push
            echo "‚úÖ Data committed and pushed."
          fi

      # 9Ô∏è‚É£ Upload artifact (for manual download)
      - name: Upload artifact (repos.csv)
        uses: actions/upload-artifact@v4
        with:
          name: github-repos
          path: data/repos.csv
          if-no-files-found: error
