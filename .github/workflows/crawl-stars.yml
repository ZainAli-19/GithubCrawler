name: Crawl Stars

on:
  workflow_dispatch:   # Manual trigger
  schedule:
    - cron: "0 0 * * *"   # ‚è∞ Run daily at midnight UTC

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 360   # allow long crawl (up to 6 hours)

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    steps:
      # 1Ô∏è‚É£ Checkout repository
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: true  # Required for committing back to repo

      # 2Ô∏è‚É£ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3Ô∏è‚É£ Install dependencies
      - name: Install dependencies
        run: pip install -r src/requirements.txt

      # 4Ô∏è‚É£ Wait for PostgreSQL
      - name: Wait for PostgreSQL to be ready
        env:
          PGPASSWORD: postgres
        run: |
          echo "‚è≥ Waiting for PostgreSQL to be ready..."
          for i in {1..20}; do
            if pg_isready -h 127.0.0.1 -p 5432 -U postgres; then
              echo "‚úÖ PostgreSQL is ready!"
              break
            else
              echo "PostgreSQL not ready yet... waiting 3s"
              sleep 3
            fi
          done

      # 5Ô∏è‚É£ Setup database schema
      - name: Setup database schema
        env:
          PGPASSWORD: postgres
        run: |
          echo "üì¶ Creating tables in PostgreSQL..."
          psql -h 127.0.0.1 -U postgres -d postgres -f src/db/migrations/001_create_tables.sql
          echo "‚úÖ Tables created successfully."

      # 6Ô∏è‚É£ Run crawler (100,000 repos)
      - name: Run crawler
        env:
          POSTGRES_HOST: 127.0.0.1
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          echo "üöÄ Running crawler for 100,000 repositories..."
          python src\crawler\crawler.py
          echo "‚úÖ Crawler completed successfully."

      # 7Ô∏è‚É£ Export data to CSV
      - name: Export database to CSV
        env:
          PGPASSWORD: postgres
        run: |
          mkdir -p data
          echo "üì§ Exporting database to data/repos.csv..."
          psql -h 127.0.0.1 -U postgres -d postgres -c "\copy (SELECT * FROM repositories) TO 'data/repos.csv' CSV HEADER"
          echo "‚úÖ Export done."

      # 8Ô∏è‚É£ Commit and push CSV to repo
      - name: Commit and push CSV results
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add data/repos.csv
          if git diff --cached --quiet; then
            echo "üü° No changes to commit."
          else
            git commit -m "üìä Auto-update: GitHub repos crawl on $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
            git push
            echo "‚úÖ Data committed and pushed to repo."
          fi

      # 9Ô∏è‚É£ Upload artifact (for manual download)
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: github-repos
          path: data/repos.csv
          if-no-files-found: error
