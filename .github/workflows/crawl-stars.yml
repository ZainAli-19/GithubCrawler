name: Crawl Stars

on:
  workflow_dispatch:    # Manual trigger from Actions tab
  schedule:
    - cron: "0 0 * * *"  # Run daily at midnight UTC

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 360   # Allow up to 6 hours for the full crawl

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
        volumes:
          - postgres-data:/var/lib/postgresql/data  # ‚úÖ Persistent DB storage between runs

    steps:
      # 1Ô∏è‚É£ Checkout repository
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: true  # Required for pushing CSV later

      # 2Ô∏è‚É£ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3Ô∏è‚É£ Install dependencies
      - name: Install dependencies
        run: pip install -r src/requirements.txt

      # 4Ô∏è‚É£ Wait for PostgreSQL
      - name: Wait for PostgreSQL to be ready
        env:
          PGPASSWORD: postgres
        run: |
          echo "‚è≥ Waiting for PostgreSQL to be ready..."
          for i in {1..20}; do
            if pg_isready -h 127.0.0.1 -p 5432 -U postgres; then
              echo "‚úÖ PostgreSQL is ready!"
              break
            else
              echo "PostgreSQL not ready yet... waiting 3s"
              sleep 3
            fi
          done

      # 5Ô∏è‚É£ Setup schema (only if not exists)
      - name: Initialize database schema
        env:
          PGPASSWORD: postgres
        run: |
          echo "üì¶ Ensuring tables exist..."
          psql -h 127.0.0.1 -U postgres -d postgres -f src/db/migrations/001_create_tables.sql
          echo "‚úÖ Schema ready."

      # 6Ô∏è‚É£ Run or resume crawl
      - name: Run crawler (resumable)
        env:
          POSTGRES_HOST: 127.0.0.1
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
          GITHUB_TOKEN: ${{ github.token }}
          PYTHONPATH: src   
        run: |
          echo "üöÄ Starting or resuming crawl..."
          python -m src.crawler.crawler
          echo "‚úÖ Crawl finished or checkpoint saved."

      # 7Ô∏è‚É£ Export CSV snapshot
      - name: Export database snapshot
        env:
          PGPASSWORD: postgres
        run: |
          mkdir -p data
          echo "üì§ Exporting repositories table..."
          psql -h 127.0.0.1 -U postgres -d postgres -c "\copy (SELECT * FROM repositories) TO 'data/repos.csv' CSV HEADER"
          echo "‚úÖ Export complete."

      # 8Ô∏è‚É£ Commit & push data
      - name: Commit and push CSV results
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add data/repos.csv
          if git diff --cached --quiet; then
            echo "üü° No new data to commit."
          else
            git commit -m "üìä Auto-update: GitHub crawl $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
            git push
            echo "‚úÖ Data committed and pushed."
          fi

      # 9Ô∏è‚É£ Upload artifact for manual download
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: github-repos
          path: data/repos.csv
          if-no-files-found: error
